{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "created by **Farah Rabie** (f.rabie@hw.ac.uk)\n",
        "***"
      ],
      "metadata": {
        "id": "3rVnICny3rKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introduction"
      ],
      "metadata": {
        "id": "q42NLP0n3tmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explore how a neural network can predict **permeability** (KLOGH) from well log measurements. We look into:\n",
        "\n",
        "*   **Preprocess** the well log data, focusing on sandstone intervals.\n",
        "*   **Examine** relationships between input features and the target using Spearman’s correlation.\n",
        "*   **Train** a feedforward neural network on processed data.\n",
        "*   **Evaluate** the trained network to see how well it captures the trends."
      ],
      "metadata": {
        "id": "24QkHa8M4jZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, we start with cloning the GeosciencePlusAI GitHub repository into Colab to access and run its code."
      ],
      "metadata": {
        "id": "7kwiSX0thxuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCSGpXl-07L4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/farah-rabie/GeosciencePlusAI.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### I$\\,\\,\\,\\,\\,\\,$Data Visualisation"
      ],
      "metadata": {
        "id": "ZgHhq4De-Ql0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's visualise the data. To do so, we import the `VisualiseWellData` class and create a visualiser instance."
      ],
      "metadata": {
        "id": "o6QfdMAWjlyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from GeosciencePlusAI.Lib.DataVisualisation import VisualiseWellData\n",
        "visualiser = VisualiseWellData()"
      ],
      "metadata": {
        "id": "VkVRcusu1fMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we list the file paths and names of all available well datasets to be used in this notebook."
      ],
      "metadata": {
        "id": "q9z2-L9348F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of well file paths\n",
        "well_files = [\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-1 B.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-4.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-5.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-11 B.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-12.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-14.csv\",\n",
        "    \"/content/GeosciencePlusAI/Data/15_9-F-15 C.csv\"\n",
        "]\n",
        "\n",
        "# List of corresponding well names\n",
        "well_names = [\n",
        "    \"15_9-F-1 B\",\n",
        "    \"15_9-F-4\",\n",
        "    \"15_9-F-5\",\n",
        "    \"15_9-F-11 B\",\n",
        "    \"15_9-F-12\",\n",
        "    \"15_9-F-14\",\n",
        "    \"15_9-F-15 C\"\n",
        "]"
      ],
      "metadata": {
        "id": "dDHDWyIN48RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### II$\\,\\,\\,\\,\\,\\,$Data Processing"
      ],
      "metadata": {
        "id": "klaxi-pq-UjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import the DataProcessing class from the `NeuralNetworkFunctions` module and create an instance called `DataProcess`. Similar to earlier exercises, this object will be used to handle the processing of the well log data."
      ],
      "metadata": {
        "id": "26ndCBVqlyBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from GeosciencePlusAI.Lib.NeuralNetworkFunctions import DataProcessing\n",
        "DataProcess = DataProcessing()"
      ],
      "metadata": {
        "id": "kAba86q31fUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define separate lists for training wells and testing wells. The training wells will be used to fit our neural networks, while the testing wells will be used to evaluate the performance of the trained networks on unseen data."
      ],
      "metadata": {
        "id": "EmJiMJUPmXM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_well_data_path = [\"/content/GeosciencePlusAI/Data/15_9-F-1 B.csv\", \"/content/GeosciencePlusAI/Data/15_9-F-4.csv\", \"/content/GeosciencePlusAI/Data/15_9-F-5.csv\"]\n",
        "test_well_data_path = [\"/content/GeosciencePlusAI/Data/15_9-F-12.csv\"]\n",
        "\n",
        "# List of wells:\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-1 B.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-4.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-5.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-11 B.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-12.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-14.csv\"\n",
        "#\"/content/GeosciencePlusAI/Data/15_9-F-15 C.csv\""
      ],
      "metadata": {
        "id": "L13xYZWb35r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use the `process_well_data()` method from the `DataProcessing` class to handle data cleaning. The data is filtered for `sandstone` intervals. We start with processing the training data."
      ],
      "metadata": {
        "id": "okWYHoAcvsli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify which well log features to include in the training dataset\n",
        "selected_columns_train = ['BVW', 'KLOGH', 'VSH', 'GR', 'NPHI', 'RHOB', 'RT', 'LITHOLOGY'] # 'LITHOLOGY' was only included for filtering\n",
        "processed_train_well_data = DataProcess.process_well_data(\n",
        "    train_well_data_path,\n",
        "    selected_columns_train,\n",
        "    method='standard',\n",
        "    train_data=True, # this indicates that this data should be used to compute the scaling parameters (mean and standard deviation for method='standard')\n",
        "    show_stats=True,\n",
        "    filter_lithology='Sandstone'\n",
        ")"
      ],
      "metadata": {
        "id": "tCMDJqlN37Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We process the testing wells next. The selected columns include the same features as in training, plus `DEPTH` for reference. As before, we filter for `sandstone` intervals and display summary statistics."
      ],
      "metadata": {
        "id": "XBG-gEjrxDpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns_test = ['DEPTH', 'BVW', 'KLOGH', 'VSH', 'GR', 'NPHI', 'RHOB', 'RT', 'LITHOLOGY'] # 'LITHOLOGY' was only included for filtering\n",
        "processed_test_well_data = DataProcess.process_well_data(\n",
        "    test_well_data_path,\n",
        "    selected_columns_test,\n",
        "    method='standard',\n",
        "    show_stats=True,\n",
        "    filter_lithology='Sandstone')"
      ],
      "metadata": {
        "id": "pSDwMOm88joJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `scale_dataframe()`, we scale the processed training and test datasets."
      ],
      "metadata": {
        "id": "-8eCHrVQyi-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling training dataset\n",
        "scaled_train_well_data = DataProcess.scale_dataframe(processed_train_well_data, show_stats=True)\n",
        "\n",
        "# scaling test dataset\n",
        "scaled_test_well_data = [] # each well has its data stored in a separate DataFrame\n",
        "for df in processed_test_well_data:\n",
        "    scaled_df = DataProcess.scale_dataframe(df, show_stats=True)\n",
        "    scaled_test_well_data.append(scaled_df)"
      ],
      "metadata": {
        "id": "ZFZqFlCf8p-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the neural network, it’s useful to examine how the input features relate to the target variable (`log_KLOGH`). We compute Spearman’s correlation using the scaled training data to capture monotonic relationships, and visualise them in a heatmap to identify which features are most strongly associated with permeability."
      ],
      "metadata": {
        "id": "jA4zMuGXLpq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Select the input features and target from scaled_train_well_data\n",
        "features = ['BVW', 'log_KLOGH', 'VSH', 'GR', 'NPHI', 'RHOB', 'log_RT']\n",
        "df_corr = scaled_train_well_data[features]\n",
        "\n",
        "# Compute Spearman correlation\n",
        "corr_matrix = df_corr.corr(method='spearman')\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "cax = ax.matshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.colorbar(cax)\n",
        "\n",
        "# Set labels\n",
        "ax.set_xticks(range(len(features)))\n",
        "ax.set_yticks(range(len(features)))\n",
        "ax.set_xticklabels(features, rotation=45, ha='left')\n",
        "ax.set_yticklabels(features)\n",
        "ax.set_title(\"Spearman's Correlation Matrix (on scaled training data)\", pad=15)\n",
        "\n",
        "# Annotate each cell with correlation value\n",
        "for (i, j), val in np.ndenumerate(corr_matrix.values):\n",
        "    ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_fDSWKCI-4-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### III$\\,\\,\\,\\,\\,\\,$Neural Network Training"
      ],
      "metadata": {
        "id": "82xbXHajEqrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the input and output arrays for the neural network. `X_train` holds the input feature(s), while `y_train` contains the target variable.\n",
        "\n",
        "Both arrays are converted to NumPy arrays, and `y_train` is reshaped to match the expected dimensions for training."
      ],
      "metadata": {
        "id": "GarpSpu-4Ugz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = scaled_train_well_data[['GR', 'VSH','RHOB', 'log_RT']].to_numpy() # Input options: 'BVW', 'log_KLOGH', 'VSH', 'GR', 'NPHI', 'RHOB', 'log_RT'\n",
        "y_train = scaled_train_well_data['log_KLOGH'].to_numpy().reshape(-1, 1) # Output: 'log_KLOGH'"
      ],
      "metadata": {
        "id": "KVf58KhX8w_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a similar manner, we loop through the test wells, creating `X_test` and `y_test` arrays for each."
      ],
      "metadata": {
        "id": "59Op5Z2a47XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_list = []\n",
        "y_test_list = []\n",
        "\n",
        "for df in scaled_test_well_data:\n",
        "    X_test = df[['GR', 'VSH','RHOB', 'log_RT']].to_numpy() # Input options: 'BVW', 'log_KLOGH', 'VSH', 'GR', 'NPHI', 'RHOB', 'log_RT'\n",
        "    y_test = df['log_KLOGH'].to_numpy().reshape(-1, 1) # Output: 'log_KLOGH'\n",
        "\n",
        "    X_test_list.append(X_test)\n",
        "    y_test_list.append(y_test)"
      ],
      "metadata": {
        "id": "X8O81RZ6aLoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shuffle the training data to randomise the order of samples. This helps prevent the neural network from learning any unintended patterns from the original sequence of the data. The `random_state` parameter ensures the shuffle is reproducible."
      ],
      "metadata": {
        "id": "RqpTdZC_5hLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = DataProcess.shuffle_data(X_train, y_train, random_state=42)"
      ],
      "metadata": {
        "id": "7-gelYXm88uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a 2D crossplot of the training data to visualise the relationship between input(s) and target. This helps assess patterns and potential correlations before training the neural network.\n",
        "\n",
        "Given how the data was filtered earlier, the plot only shows `sandstone` intervals."
      ],
      "metadata": {
        "id": "u_bdtSrl6HJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualiser.crossplot_2D(\n",
        "    well_name=\"Training Data\",\n",
        "    df=scaled_train_well_data,\n",
        "    x_col=\"log_RT\", # options: 'GR', 'VSH','RHOB', 'log_RT'\n",
        "    y_col=\"log_KLOGH\",\n",
        "    color_col=\"LITHOLOGY\",\n",
        "    filter_lithology=\"Sandstone\"\n",
        ")"
      ],
      "metadata": {
        "id": "HkexOD4jMymO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the `FeedforwardNeuralNetwork` class, which provides a simple fully connected neural network implementation for regression tasks. This will be used to model the relationship between NPHI and log_KLOGH."
      ],
      "metadata": {
        "id": "fOQUKTJBE5bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from GeosciencePlusAI.Lib.NeuralNetworkFunctions import FeedforwardNeuralNetwork"
      ],
      "metadata": {
        "id": "H5qcvYQS9IYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialise the feedforward neural network, as follows:\n",
        "*   `input_NN` defines the number of input features (here, 1 for NPHI).\n",
        "*   `hidden_NN` specifies three hidden layers, each with 48 neurons.\n",
        "*   `activation_NN='relu'` applies the ReLU activation function to the hidden layers.\n",
        "*   `output_NN=[1]` defines a single output neuron for predicting log_KLOGH.\n",
        "*   `output_names` labels the output variable for reference.\n",
        "\n",
        "This sets up the network architecture before training."
      ],
      "metadata": {
        "id": "TU9ueSklFI-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeedforwardNeuralNetwork(\n",
        "    input_NN=[X_train.shape[1]],\n",
        "    hidden_NN=[48, 48, 48],\n",
        "    activation_NN='relu',\n",
        "    output_NN=[1],\n",
        "    output_names=['log_KLOGH']\n",
        ")"
      ],
      "metadata": {
        "id": "ElD_rrzD-kgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compile the neural network. We specify the following training parameters:\n",
        "\n",
        "*   `l_rate` sets the learning rate for the optimiser.\n",
        "*   `beta_1` and `beta_2` are momentum parameters for the Adam optimiser.\n",
        "*   `epsilon` is a small value to prevent division by zero.\n",
        "*   `loss_type='mae'` uses mean absolute error as the loss function, which is appropriate for regression tasks.\n",
        "\n",
        "This prepares the model for training."
      ],
      "metadata": {
        "id": "pQzmjT-4GHSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compileNNFeedforward(\n",
        "    l_rate=1e-3, beta_1=0.99, beta_2=0.999, epsilon=1e-7,\n",
        "    loss_type='mse' # 'l1l2', 'mae', or 'mse'\n",
        ")"
      ],
      "metadata": {
        "id": "Il7KIYF9-_Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the neural network on the training data with a *validation split* of 20%. This means 20% of the training data is held out during training to monitor the model's performance on unseen samples. Training runs for 200 `epochs` with a `batch size` of 32, and the progress (including training and validation loss) is stored in `history`."
      ],
      "metadata": {
        "id": "7cJGwtBlGlMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.train(X_train, y_train, epochs=200, batch_size=32, verbose=2,\n",
        "                      validation_split=0.2)"
      ],
      "metadata": {
        "id": "ty_cupza_BUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the training and validation loss over epochs to visualize how the performance of the model evolves. This helps identify underfitting, overfitting, or whether additional training might improve results."
      ],
      "metadata": {
        "id": "MCISr2zlIXvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.plot_training_loss(history)"
      ],
      "metadata": {
        "id": "qkDzgJLQ_UGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate predictions for each test well using the trained neural network. For each set of test inputs, the network predicts `log_KLOGH`, and the results are stored in a list for later evaluation."
      ],
      "metadata": {
        "id": "30E6PIQqIp45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test = []\n",
        "for X_test in X_test_list:\n",
        "    y_pred = model.NN_Feedforward.predict(X_test, verbose=0)\n",
        "    predictions_test.append(y_pred)"
      ],
      "metadata": {
        "id": "_oYpRkCwaRff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot predicted `log_KLOGH` values against the actual measurements for each test well. The red dashed line represents the ideal 1:1 fit - points close to this line indicate accurate predictions, while deviations highlight areas where the model under- or over-predicts. This provides a quick visual assessment of overall model performance."
      ],
      "metadata": {
        "id": "ISRuvg4lhsCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "for i, (X_test, y_test, y_pred) in enumerate(zip(X_test_list, y_test_list, predictions_test)):\n",
        "    y_pred = y_pred.flatten()      # Flatten predicted array\n",
        "    y_test = y_test.flatten()      # Flatten true array\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(y_test, y_pred, c='blue', alpha=0.6, edgecolors='k')\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal Fit')\n",
        "    plt.xlabel('Actual log_KLOGH')\n",
        "    plt.ylabel('Predicted log_KLOGH')\n",
        "    plt.title(f'Test Well {i+1}: Predicted vs Actual')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M7Ro4FksaxrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualise how the predicted `log_KLOGH` compares to the actual values for each test well in relation to the input feature `NPHI`. Side-by-side scatter plots allow us to see whether the model captures the trends present in the training data and highlight any deviations."
      ],
      "metadata": {
        "id": "zrO84eRnhs0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of input features in X_test (must match the order used in training)\n",
        "input_features = ['GR', 'VSH','RHOB', 'log_RT']\n",
        "feature_name = 'GR'  # Feature to plot on x-axis\n",
        "\n",
        "# Get index of the selected feature in X_test\n",
        "feature_index = input_features.index(feature_name)\n",
        "\n",
        "for i, (X_test, y_test, y_pred) in enumerate(zip(X_test_list, y_test_list, predictions_test)):\n",
        "    y_pred = y_pred.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    # Select the feature to plot\n",
        "    if X_test.ndim == 1 or X_test.shape[1] == 1:\n",
        "        feature_data = X_test.flatten()  # single feature\n",
        "    else:\n",
        "        feature_data = X_test[:, feature_index]  # multiple features\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
        "\n",
        "    # Actual values\n",
        "    axs[0].scatter(feature_data, y_test, c='blue', alpha=0.6, edgecolors='k')\n",
        "    axs[0].set_xlabel(feature_name)\n",
        "    axs[0].set_ylabel('log_KLOGH')\n",
        "    axs[0].set_title(f'Test Well {i+1} - Actual')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Predicted values\n",
        "    axs[1].scatter(feature_data, y_pred, c='orange', alpha=0.6, edgecolors='k')\n",
        "    axs[1].set_xlabel(feature_name)\n",
        "    axs[1].set_title(f'Test Well {i+1} - Predicted')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AkfprGe4hh6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also compare the neural network’s predictions of log_KLOGH with the true values along the well depth."
      ],
      "metadata": {
        "id": "W0uYnmFqSoEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (df, y_pred) in enumerate(zip(scaled_test_well_data, predictions_test)):\n",
        "    depth = df['DEPTH'].values\n",
        "    y_true = df['log_KLOGH'].values\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 12), sharey=True)\n",
        "\n",
        "    # True values\n",
        "    axs[0].plot(y_true, depth, color='blue')\n",
        "    axs[0].set_xlabel('log_KLOGH')\n",
        "    axs[0].set_ylabel('Depth')\n",
        "    axs[0].set_title(f'Test Well {i+1} - True')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Predicted values\n",
        "    axs[1].plot(y_pred_flat, depth, color='orange')\n",
        "    axs[1].set_xlabel('log_KLOGH')\n",
        "    axs[1].set_title(f'Test Well {i+1} - Predicted')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    plt.gca().invert_yaxis()  # Depth increases downward\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4uGcanPQ9hIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgd07WyKRdwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}